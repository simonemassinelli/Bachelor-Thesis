Since the calculation of the Cpk index relies on the assumption that the data follow a normal distribution, the first step in this phase of the analysis was to visually inspect the shape of the distributions. This was done with two main goals: to see if the data appeared approximately normal, and to evaluate whether removing the trend improved their adherence to a Gaussian distribution. Company already assumes normality of the production data, unless specific exceptions (previously discussed in subsection 5.3). Starting from this premise, if detrending brings the distribution closer to a normal shape, it would support the hypothesis that the trend observed in the raw data is caused by gradual changes in the behavior of the machining tool.

To conduct this analysis, the data were once again divided into the time intervals of the machines lifecyles, and also aggregated to capture general distribution. For each case, two histograms with corresponding kernel density estimates (KDE) were plotted : one before and one after detrending.

The following Python script was used to generate the plots:

\end{flushleft}

```{python}
from scipy.stats import gaussian_kde
from plotly.subplots import make_subplots

# Function to calculate KDE (Kernel Density Estimation)
def get_kde(x_vals, data):
    kde = gaussian_kde(data)  # Fit a kernel density estimator to the data
    return kde(x_vals)  # Evaluate KDE at the points x_vals

# Concatenate the data for all machines (before detrending)
all_original = qc_rows['deviation']
# Concatenate the data after detrending
all_detrended = pd.concat([detrended_m1, detrended_m2, detrended_m3],
                ignore_index=True)

# List of datasets with names and colors for each plot
datasets = [(all_original, "All Data - Original", 'royalblue'),
    (all_detrended, "All Data - Detrended", 'firebrick'),
    (df_machine1['deviation'], "Machine 1 - Original", 'gray'),
    (detrended_m1, "Machine 1 - Detrended", 'darkgray'),
    (df_machine2['deviation'], "Machine 2 - Original", 'orange'),
    (detrended_m2, "Machine 2 - Detrended", 'darkorange'),
    (df_machine3['deviation'], "Machine 3 - Original", 'lightgreen'),
    (detrended_m3, "Machine 3 - Detrended", 'green')]

# Create subplots layout with 8 rows and 1 column
fig = make_subplots(rows=8, cols=1,
    subplot_titles=[name for _, name, _ in datasets])  # Set titles for each subplot

# Add histogram and KDE for each dataset
for i, (data, title, color) in enumerate(datasets):
    row = i + 1  # Determine which row to plot the data on

    # Calculate KDE
    # Create a range of x values for the KDE
    x_vals = np.linspace(data.min(), data.max(), 500)
    kde_vals = get_kde(x_vals, data)  # Get the KDE values for the given data

    # Add histogram to the plot
    fig.add_trace(go.Histogram(x=data,
        histnorm='probability density',
        name='Histogram',
        marker_color=color,
        opacity=0.6,
        xbins=dict(size=0.001),  # Bin size
        showlegend=False),  # Don't show legend for the histogram
        row=row, col=1)

    # Add KDE curve to the plot
    fig.add_trace(go.Scatter(
        x=x_vals,
        y=kde_vals,
        mode='lines',  # Line plot for KDE
        line=dict(color='white', width=2),
        name='KDE',
        showlegend=False),  # Don't show legend for the KDE curve
        row=row, col=1)

# Global layout settings
fig.update_layout(height=2600,
    title="Distributions with Histogram and KDE Curve",  # Title of the plot
    bargap=0.1)  # Gap between bars in the histogram


# Set axis labels for each subplot
for r in range(1, 9):
    fig.update_yaxes(title_text="Density", row=r, col=1)
    fig.update_xaxes(title_text="Deviation", row=r, col=1)

# Save as PNG for LaTeX/PDF output
pio.write_image(fig, "figures/norm-plot.png", format="png")
```

\begin{flushleft}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth, height=\textheight]{figures/norm1-plot.png}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth, height=\textheight]{figures/norm2-plot.png}
\end{figure}

It is important to note that the y-axis does not represent the absolute frequency of the measurements, but rather the probability density. This is because the histogram has been normalized using the probability density setting, which ensures that the area under the histogram sums to 1. This allows for a meaningful comparison with the KDE curve.

The results clearly show that, in all four cases (three intervals and the full dataset), the data appear more normally distributed after the trend is removed. This strongly confirm the presence of the trends in the production processes.

We can see that none of the distributions are perfectly Gaussian, particularly in the first time interval. In that case, a suspicious measurement strongly influences the trend estimation. However, it cannot be treated as a statistical outlier, since its Mahalanobis distance was not as great as the ones of the change dates for example. That is because the process at that time was simply operating with higher variance. Anyway, the presence of this data point, likely caused the estimated trend line to shift downward compared to its actual position. As a result, even after detrending, the distribution remains skewed toward the right.

Despite these imperfections, it is important to consider the whole context: the datasets are small, derived from different production lots, and span a long time period that includes many process changes and events. Under these conditions, it is reasonable to accept that the distributions are not perfectly normal, and the assumption of normality can still be considered valid for the purposes of Cpk calculation.

Once this assumption has been validated, the next challenge was to determine how to compute the Cpk index in a meaningful way, given the temporal distance of the data points. Standard Cpk estimation requires at least 30 data points to provide a stable estimate. In our computation, consider 30 data points would lead to a very high difference between the first and the last production dates, while computing a weekly Cpk, for example, would lead to have just few data points, not enough to compute the index. I needed to find a compromise and I consequently choose to compute the daily Cpk using every time the data of the last month, in which the production days are 20 or little more. This corresponds to have about 20/25 measurements each time, and I imposed that 20 should be the minimum number accepted with a filter.

However, applying a simple rolling calculation across the last 30 days measurements, would ignore the fact that these data points originate from batches spread across a wide time window. It seemed more reasonable to assign a weight to each measurement based on its recency, in order to obtain a smoother and more accurate estimate of the process capability at any given day.

Among the possible weighting strategies, I choose an exponential decay function :

\vspace{0.7cm}

\[
w(t) = e^{-At}
\]

\vspace{0.7cm}

where $t$ is the time elapsed in days from the measurement to the current day, and $A$ is a decay parameter. The value of $A$ was chosen arbitrarily in this analysis, but the optimal one can be find by analyzing how the Cpk truly varies from one production lot to another. For instance, I choose $A$ = 0.15, which results in an exponential daily drop of around 13.93% in the weight assigned to past measurements.

The function and a plot of the weight curve are provided below to illustrate how this approach works:

\end{flushleft}

```{python}
# Function to assign weights
def weight(time_distance, A):
    # Calculate the weight using an exponential decay function
    # A represents the decay rate
    time_distance = np.array(time_distance)  # Convert input to NumPy array
    return np.exp(-A * time_distance)  # Exponential decay weighting

# Create a list representing time distances from 0 to 15
dists = [i for i in range(31)]  # Time distances from 0 to 30 days

# Apply the weight function to generate z-values for the 3D surface plot
weights = weight(dists, 0.15)  # Compute weights with decay rate A

# Create a 3D surface plot using Plotly
fig = go.Figure(data=go.Scatter(x=dists, y=weights))  # Plot weights vs time distance

# Customize the layout with titles for the axes and the plot
fig.update_layout(title='Weight based on time distance',  # Plot title
        xaxis_title='Days',  # X-axis label
        yaxis_title='Weight')  # Y-axis label

# Save as PNG for LaTeX/PDF output
pio.write_image(fig, "figures/w-plot.png", format="png")
```

\begin{flushleft}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/w-plot.png}
\end{figure}

At this point, the final tasks consist of calculating the Cpk index for each day of production using the weighting method previously described, and then analyzing how this index changes over time.

These two scripts were created, the first one to compute the index and the second one to visualize the results :

\end{flushleft}

```{python}
# Define parameters for rolling window, minimum data points, and exponential decay
window_size = 30  # Rolling time window size (in days)
min_measurements = 20  # Minimum number of measurements required to calculate Cpk
expected_value = qc_rows['nominal'].iloc[0]  # Extract the expected nominal value
max_value = expected_value + UT  # Compute the upper tolerance limit
min_value = expected_value + LT  # Compute the lower tolerance limit
A = 0.15  # Decay factor used in the weighting function

# Combine all machine datasets for processing
all_data = [df_machine1, df_machine2, df_machine3]
for df in all_data:
    # Add a column with only the date (no time) from datetime
    df['day'] = df['date_c'].dt.date  # Extract date component
    # Flag the last measurement of each production day
    df['last_measurement'] = df['day'] != df['day'].shift(-1)
    # Initialize the Cpk column with NaN values
    df['cpk'] = np.nan  # Prepare column for Cpk results

    # Count how many production days are present in the dataset
    production_days = df['last_measurement'].sum()
    # Filter only the last measurement entries for each day
    last_measurements = df.loc[df['last_measurement'] == True]  # Daily endpoints

    # Loop through each production day to compute Cpk
    for i in range(production_days):
        # Select the current day's last measurement
        production_day = last_measurements.iloc[i]
        date = production_day['day']  # Extract the date

        # Define the time window start (rolling back in time)
        window_start = date - pd.Timedelta(days=window_size - 1)
        # Filter the dataset to include only data within the rolling window
        window = df[(df['day'] >= window_start) & (df['day'] <= date)]
        total_measurements = len(window)  # Number of measurements in the window

        # Check if the window contains enough data to calculate Cpk
        if total_measurements < min_measurements:
            df.loc[last_measurements.index[i], 'cpk'] = np.nan  # Not enough data
        else:
            # Initialize the list of weights
            alfa = []
            for j in range(len(window)):
                # Compute the time distance between each data point and the current day
                td = (date - window['day'].iloc[j]).days  # Time difference in days
                # Apply the exponential decay weighting function
                alfa.append(weight(td, A))  # Calculate weight

            # Normalize weights
            norm_factor = sum(alfa)  # Sum of weights for normalization
            normalized_alfa = [a / norm_factor for a in alfa]  # Normalize each weight
            # Compute weighted sum for mean calculation
            day_term = [window['actual'].iloc[k] * normalized_alfa[k]
                       for k in range(len(window))]  # Weighted values

            # Calculate weighted mean
            mean = sum(day_term)  # Final weighted mean
            # Compute variance and standard deviation
            var = np.sum((window['actual'] - mean)**2) / total_measurements
            sigma = np.sqrt(var)

            # Calculate Cpk for both upper and lower limits
            cpk_upper = (max_value - mean) / (3 * sigma)  # Upper process capability
            cpk_lower = (mean - min_value) / (3 * sigma)  # Lower process capability
            cpk = min(cpk_upper, cpk_lower)  # Take the worst-case (minimum) value

            # Assign the Cpk value to the corresponding row in the DataFrame
            df.loc[last_measurements.index[i], 'cpk'] = cpk  # Save the result
```

```{python}
# Define Cpk thresholds for warning and critical levels
cpk_threshold_warning = 1.33  # Warning threshold for Cpk
cpk_threshold_critical = 1.00  # Critical threshold for Cpk

# Create a figure to visualize the Cpk values over time
fig = go.Figure()

# Iterate through the datasets for each machine and generate the plot
for idx, df in enumerate(all_data):
    df_valid = df.dropna(subset=['cpk'])  # Remove rows where Cpk is NaN

    # List to store colors for markers based on Cpk values
    colors = []
    for cpk in df_valid['cpk']:
        if cpk < cpk_threshold_critical:
            colors.append('red')  # Cpk below the critical threshold
        elif cpk < cpk_threshold_warning:
            colors.append('orange')  # Cpk between the critical and warning thresholds
        else:
            colors.append('green')  # Cpk above the warning threshold

    # Add a trace to the figure using the date and Cpk values
    fig.add_trace(go.Scatter(x=df_valid['day'], y=df_valid['cpk'],
                             mode='lines+markers',
                             marker=dict(color=colors, size=6),
                             line=dict(width=2, dash='solid'),
                             name=f'Machine {idx+1}'))

# Add horizontal lines to indicate the critical and warning thresholds
fig.add_hline(y=cpk_threshold_critical, line_dash='dash', line_color='red')
fig.add_hline(y=cpk_threshold_warning, line_dash='dash', line_color='orange')

# Add rectangles to visually highlight the critical, warning, and normal regions
fig.add_hrect(y0=cpk_threshold_critical, y1=cpk_threshold_warning,
              fillcolor='rgba(255, 165, 0, 0.2)', line_width=0)
fig.add_hrect(y0=0, y1=cpk_threshold_critical, fillcolor='rgba(255, 0, 0, 0.2)',
              line_width=0)
fig.add_hrect(y0=cpk_threshold_warning, y1=12, fillcolor='rgba(0, 255, 0, 0.2)',
              line_width=0)

# Update the layout of the figure
fig.update_layout(title='Cpk over Time',
                  xaxis_title='Date',
                  yaxis_title='Cpk',
                  xaxis=dict(showgrid=True),
                  yaxis=dict(showgrid=True, zeroline=True, zerolinecolor='black'))

# Add annotations to explain the threshold lines
fig.add_annotation(
    text='- - - → Cpk = 1.33 → Warning',
    xref='paper', yref='paper',  # Coordinates relative to the plot
    x=0.95, y=0.9,  # Position (top right)
    showarrow=False,
    font=dict(size=14, color='orange'),  # Orange text for warning
    borderwidth=1)

fig.add_annotation(
    text='- - - → Cpk = 1.00 → Critical',
    xref='paper', yref='paper',
    x=0.95, y=0.75,  # Slightly lower position
    showarrow=False,
    font=dict(size=14, color='red'),  # Red text for critical
    borderwidth=1)

# Save as PNG for LaTeX/PDF output
pio.write_image(fig, "figures/cpk-plot.png", format="png")
```

\begin{flushleft}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/cpk-plot.png}
\end{figure}

In the graph, we can see how the Cpk index changes over time for each of the three machines. This gives us a general idea of how the capability of each machine has evolved during the production period.

However, just looking at the index itself, is not easy to understand how quickly things are improving or getting worse. For this reason, I also calculated how much the Cpk changes from one day to the next, actually computing a sort of derivative. This allows us to better understand how a machine loses or gains capability over time.

\end{flushleft}

```{python}
# Define thresholds for good and warning levels
good = 0
warning = -0.3

# Create the Plotly figure for daily Cpk difference
fig_daily_diff = go.Figure()

# Iterate through the dataset, calculating the daily Cpk change
for idx, df in enumerate(all_data):
    df_valid = df.dropna(subset=['cpk'])
    daily_slope = [0]  # Start with 0 for the first point

    for i in range(1, len(df_valid)):
        cpk_diff = df_valid['cpk'].iloc[i] - df_valid['cpk'].iloc[i-1]
        days_diff = (df_valid['day'].iloc[i] - df_valid['day'].iloc[i-1]).days
        daily_slope.append(cpk_diff / days_diff if days_diff != 0 else 0)

    # Add the trace for daily Cpk change
    fig_daily_diff.add_trace(go.Scatter(
        x=df_valid['day'],
        y=daily_slope,
        mode='lines',
        name=f'Machine {idx+1}'))

# Add threshold lines
fig_daily_diff.add_hline(y=good, line=dict(color='green', dash='dash'))
fig_daily_diff.add_hline(y=warning, line=dict(color='yellow', dash='dash'))

# Add highlighted regions
fig_daily_diff.add_hrect(y0=good, y1=2, fillcolor='rgba(0, 255, 0, 0.2)', line_width=0)
fig_daily_diff.add_hrect(y0=warning, y1=good, fillcolor='rgba(255, 255, 0, 0.2)',
                         line_width=0)
fig_daily_diff.add_hrect(y0=-2, y1=warning, fillcolor='rgba(255, 0, 0, 0.2)',
                         line_width=0)

# Update layout
fig_daily_diff.update_layout(
    title='Daily Change in Cpk Index',
    xaxis_title='Date',
    yaxis_title='Daily Cpk Difference')

# Save as PNG for LaTeX/PDF output
pio.write_image(fig_daily_diff, "figures/der-plot.png", format="png")
```

\begin{flushleft}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/der-plot.png}
\end{figure}

From both graphs, the Cpk evolution and the daily Cpk difference, it is possible to extract several meaningful insights. These findings represent exactly the kind of conclusions I aimed to reach through this analysis and all of these aspects will be discussed in detail in the next and final chapter.

\end{flushleft}

\newpage

\begin{flushleft}
\makebox[\linewidth]{%
  \makebox[0pt][l]{\textbf{9}}%
  \makebox[\linewidth][c]{\textbf{Discussion and Conclusions}}%
}\\[1.5cm]

\textbf{9.1 Discussion of Results} \\[1.5cm]

\textbf{9.2 Opportunities for Improving the Analysis} \\[1.5cm]

\end{flushleft}



Riscrivi questo testo come se fosse stato scritto da uno studente universitario (non madrelingua ma competente), in stile accademico semplice. Mantieni chiarezza e coerenza, ma evita frasi troppo fluide o bilanciate. Usa qualche struttura leggermente ridondante, ripetizioni lievi, esitazioni implicite, oppure transizioni non perfettamente levigate. Il tono deve essere serio e tecnico, ma il ritmo deve sembrare umano, come chi scrive bene ma senza editing perfetto. Non usare espressioni troppo raffinate o formule da AI. Fai sembrare il testo autentico e spontaneo.

ora devo fare le conclusioni, il primo sottocapitolo è il 9.1 Discussion of Results, che il mio relatore mi ha consigliato di affrontare proprio come una discussione dei risultati ottenuti piu che un elenco dei risultati. in questo sottocapitolo voglio inanzitutto risottolineare gli obiettivi della tesi anche se gia spiegati nella parte iniziale : i possibili vantaggi nel monitorare l'indice cpk di un qualsiasi strumento di lavorazione piuttosto che calcolarlo solo durante i processi di standardizzazione